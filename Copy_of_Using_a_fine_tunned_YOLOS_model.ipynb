{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabassegio/Learning_ROS2/blob/developing/Copy_of_Using_a_fine_tunned_YOLOS_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-13T18:30:19.035920Z",
          "start_time": "2023-06-13T18:30:19.035100Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0oYeak95D7J",
        "outputId": "5e496a59-652e-49f6-8c92-bc589b69d70c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: colorthief in /usr/local/lib/python3.10/dist-packages (0.2.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from colorthief) (8.4.0)\n",
            "--2023-06-27 02:19:24--  https://github.com/JeffTrain/selfie/raw/master/shape_predictor_68_face_landmarks.dat\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/JeffTrain/selfie/master/shape_predictor_68_face_landmarks.dat [following]\n",
            "--2023-06-27 02:19:25--  https://raw.githubusercontent.com/JeffTrain/selfie/master/shape_predictor_68_face_landmarks.dat\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 99693937 (95M) [application/octet-stream]\n",
            "Saving to: ‘shape_predictor_68_face_landmarks.dat.1’\n",
            "\n",
            "shape_predictor_68_ 100%[===================>]  95.08M   202MB/s    in 0.5s    \n",
            "\n",
            "2023-06-27 02:19:26 (202 MB/s) - ‘shape_predictor_68_face_landmarks.dat.1’ saved [99693937/99693937]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install colorthief\n",
        "!wget -nd https://github.com/JeffTrain/selfie/raw/master/shape_predictor_68_face_landmarks.dat\n",
        "from PIL import Image\n",
        "from transformers import YolosFeatureExtractor, YolosForObjectDetection\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "from colorthief import ColorThief\n",
        "import webcolors\n",
        "import glob\n",
        "import cv2\n",
        "import dlib\n",
        "import numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-13T18:30:19.588891Z",
          "start_time": "2023-06-13T18:30:19.578052Z"
        },
        "id": "sUeHj6LP5D7N"
      },
      "outputs": [],
      "source": [
        "# This is the order of the categories list. NO NOT CHANGE. Just for visualization purposes\n",
        "IMAGE_PATH = \"image.jpg\"\n",
        "cats = ['shirt, blouse', 'top, t-shirt, sweatshirt', 'sweater', 'cardigan', 'jacket', 'vest', 'pants', 'shorts', 'skirt', 'coat', 'dress', 'jumpsuit', 'cape', 'glasses', 'hat', 'headband, head covering, hair accessory', 'tie', 'glove', 'watch', 'belt', 'leg warmer', 'tights, stockings', 'sock', 'shoe', 'bag, wallet', 'scarf', 'umbrella', 'hood', 'collar', 'lapel', 'epaulette', 'sleeve', 'pocket', 'neckline', 'buckle', 'zipper', 'applique', 'bead', 'bow', 'flower', 'fringe', 'ribbon', 'rivet', 'ruffle', 'sequin', 'tassel']\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ifglasses(path):\n",
        "    detector = dlib.get_frontal_face_detector()\n",
        "    predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
        "    img = dlib.load_rgb_image(path)\n",
        "    rect = detector(img)[0]\n",
        "    sp = predictor(img, rect)\n",
        "    landmarks = np.array([[p.x, p.y] for p in sp.parts()])\n",
        "    nose_bridge_x = []\n",
        "    nose_bridge_y = []\n",
        "    for i in [28,29,30,31,33,34,35]:\n",
        "        nose_bridge_x.append(landmarks[i][0])\n",
        "        nose_bridge_y.append(landmarks[i][1])\n",
        "\n",
        "    ### x_min and x_max\n",
        "    x_min = min(nose_bridge_x)\n",
        "    x_max = max(nose_bridge_x)### ymin (from top eyebrow coordinate),  ymax\n",
        "    y_min = landmarks[20][1]\n",
        "    y_max = landmarks[31][1]\n",
        "    img2 = Image.open(path)\n",
        "    img2 = img2.crop((x_min,y_min,x_max,y_max))\n",
        "\n",
        "    img_blur = cv2.GaussianBlur(np.array(img2),(3,3), sigmaX=0, sigmaY=0)\n",
        "    edges = cv2.Canny(image =img_blur, threshold1=100, threshold2=200)\n",
        "\n",
        "    #center strip\n",
        "    edges_center = edges.T[(int(len(edges.T)/2))]\n",
        "\n",
        "    if 255 in edges_center:\n",
        "        return 'Glasses'\n",
        "    else:\n",
        "        return 'No glasses'"
      ],
      "metadata": {
        "id": "j8znquTKY6RX"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "outputs": [],
      "source": [
        "def fix_channels(t):\n",
        "    \"\"\"\n",
        "    Some images may have 4 channels (transparent images) or just 1 channel (black and white images), in order to let the images have only 3 channels. I am going to remove the fourth channel in transparent images and stack the single channel in back and white images.\n",
        "    :param t: Tensor-like image\n",
        "    :return: Tensor-like image with three channels\n",
        "    \"\"\"\n",
        "    if len(t.shape) == 2:\n",
        "        return ToPILImage()(torch.stack([t for i in (0, 0, 0)]))\n",
        "    if t.shape[0] == 4:\n",
        "        return ToPILImage()(t[:3])\n",
        "    if t.shape[0] == 1:\n",
        "        return ToPILImage()(torch.stack([t[0] for i in (0, 0, 0)]))\n",
        "    return ToPILImage()(t)\n",
        "def idx_to_text(i):\n",
        "    return str(cats[i])"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-13T18:30:19.907746Z",
          "start_time": "2023-06-13T18:30:19.897407Z"
        },
        "id": "aAP6gaCY5D7N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-13T18:30:20.312733Z",
          "start_time": "2023-06-13T18:30:20.295385Z"
        },
        "id": "6NoPEecG5D7P"
      },
      "outputs": [],
      "source": [
        "# Random colors used for visualization\n",
        "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
        "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
        "\n",
        "# for output bounding box post-processing\n",
        "def box_cxcywh_to_xyxy(x):\n",
        "    x_c, y_c, w, h = x.unbind(1)\n",
        "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
        "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
        "    return torch.stack(b, dim=1)\n",
        "\n",
        "def rescale_bboxes(out_bbox, size):\n",
        "    img_w, img_h = size\n",
        "    b = box_cxcywh_to_xyxy(out_bbox)\n",
        "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
        "    return b\n",
        "\n",
        "def plot_results(pil_img, prob, boxes):\n",
        "    detec = []\n",
        "    colors = COLORS * 100\n",
        "    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), colors):\n",
        "        cl = p.argmax()\n",
        "        if cl<29:\n",
        "          if 'pants' in idx_to_text(cl):\n",
        "            xmin=(xmax+xmin)/2\n",
        "          img = pil_img.crop((xmin+20,ymin+20,xmax-20,ymax-20))\n",
        "          img.save('results/'+idx_to_text(cl)+'.png')\n",
        "          detec.append([idx_to_text(cl),[int(xmin),int(ymax),int(xmax),int(ymin)]])\n",
        "    return detec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-13T18:30:20.578229Z",
          "start_time": "2023-06-13T18:30:20.572034Z"
        },
        "id": "VTAwwcLQ5D7R"
      },
      "outputs": [],
      "source": [
        "def visualize_predictions(image, outputs, threshold=0.8):\n",
        "    # keep only predictions with confidence >= threshold\n",
        "    probas = outputs.logits.softmax(-1)[0, :, :-1]\n",
        "    keep = probas.max(-1).values > threshold\n",
        "\n",
        "    # convert predicted boxes from [0; 1] to image scales\n",
        "    bboxes_scaled = rescale_bboxes(outputs.pred_boxes[0, keep].cpu(), image.size)\n",
        "\n",
        "\n",
        "    # plot results\n",
        "    return plot_results(image, probas[keep], bboxes_scaled)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def colorName(path):\n",
        "    color_thief = ColorThief(path)\n",
        "\n",
        "    dominant_color = color_thief.get_color()\n",
        "    closest_color = None\n",
        "    min_distance = float('inf')\n",
        "    for color_name, rgb in webcolors.CSS3_NAMES_TO_HEX.items():\n",
        "        css3_rgb = webcolors.hex_to_rgb(rgb)\n",
        "\n",
        "        distance =delta_e_cie76(dominant_color, css3_rgb)\n",
        "\n",
        "        if distance < min_distance:\n",
        "            min_distance = distance\n",
        "            closest_color = color_name\n",
        "    return closest_color\n",
        "\n",
        "def delta_e_cie76(color1, color2):\n",
        "    r_mean = (color1[0] + color2[0]) / 2\n",
        "    delta_r = color1[0] - color2[0]\n",
        "    delta_g = color1[1] - color2[1]\n",
        "    delta_b = color1[2] - color2[2]\n",
        "    return ((2 + (r_mean / 256)) * delta_r ** 2 + 4 * delta_g ** 2 + (2 + ((255 - r_mean) / 256)) * delta_b ** 2) ** 0.5\n"
      ],
      "metadata": {
        "id": "6QW-hCPjbNFc"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-13T18:30:22.063798Z",
          "start_time": "2023-06-13T18:30:21.038746Z"
        },
        "id": "a-F18JvV5D7T"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"valentinafeve/yolos-fashionpedia\"\n",
        "feature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-small')\n",
        "model = YolosForObjectDetection.from_pretrained(MODEL_NAME)\n",
        "image = Image.open(open(IMAGE_PATH, \"rb\"))\n",
        "image = fix_channels(ToTensor()(image))\n",
        "image = image.resize((600, 800))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-13T18:30:28.074124Z",
          "start_time": "2023-06-13T18:30:23.048473Z"
        },
        "id": "4IkVHNWD5D7V"
      },
      "outputs": [],
      "source": [
        "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-13T18:30:28.856962Z",
          "start_time": "2023-06-13T18:30:27.963754Z"
        },
        "id": "O60mLIe45D7W",
        "outputId": "d9d6f41c-e162-40c2-a15a-eb0cf75a95b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['results/bag, wallet.png', 'darkslategray'], ['results/coat.png', 'silver'], ['results/tights, stockings.png', 'darkslategray'], ['results/shoe.png', 'black'], ['results/collar.png', 'lightgray']]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-c3b954312c5d>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mbody_colors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclothes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody_colors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mifglasses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMAGE_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-34-859b14e1f04b>\u001b[0m in \u001b[0;36mifglasses\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mifglasses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frontal_face_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_predictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shape_predictor_68_face_landmarks.dat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_rgb_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error deserializing object of type int"
          ]
        }
      ],
      "source": [
        "detec = visualize_predictions(image, outputs, threshold=0.5)\n",
        "body_colors =[]\n",
        "for clothes in glob.glob('results/*'):\n",
        "  color = colorName(clothes)\n",
        "  body_colors.append([clothes,color])\n",
        "print(body_colors)\n",
        "print(ifglasses(IMAGE_PATH))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}